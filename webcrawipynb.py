# -*- coding: utf-8 -*-
"""webcrawipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CPCsRk1CFGX4qEpi6k4kPFzYTQE6dYM8
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import time
import pandas as pd
import re
import os
import urllib.parse
import json


class CollegeNavigatorScraper:
    def __init__(self, driver_path="C:/chromedriver/chromedriver.exe"):
        """Initialize the scraper with configuration"""
        self.driver_path = driver_path
        self.driver = None
        self.base_url = "https://nces.ed.gov/collegenavigator/"
        self.wait_time = 15  # seconds
        self.search_terms = []
        self.keywords = []

        # Single dataframe to store all data
        self.all_data = []

        # Store general institution info
        self.institution_info = {}

    def run(self):
        try:
            self._setup_driver()

            # Get user input
            institution_input = self._get_user_input()
            # Make them available everywhere
            self.search_terms = institution_input["search_terms"]
            self.keywords = institution_input["keywords"]

            all_dfs = {}  # to store each institution's dataframe

            for name in institution_input["names"]:
                print(f"\n====== Processing: {name} ======")
                self.all_data = []  # Reset for each institution
                self.institution_info = {}

                # Search and extract data
                self._search_institution(name)
                if not self._select_institution_from_results():
                    print(f"No institution selected for {name}. Skipping.")
                    continue

                self._wait_for_page_load()
                self._extract_general_info()

                inst_input_copy = institution_input.copy()
                inst_input_copy["name"] = name
                inst_input_copy["clean_name"] = self._clean_filename(name)

                # ----------- Always expand all sections BEFORE processing -----------------
                self._expand_all_sections()
                # -------------------------------------------------------------------------

                self._process_institution_page(inst_input_copy)

                # Convert this institution's data to DataFrame and store
                df = pd.DataFrame(self.all_data)
                all_dfs[name] = df

            # Save all collected data to Excel with multiple sheets
            self._save_multiple_excel_data(all_dfs, institution_input)

        except Exception as e:
            print(f"An error occurred: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self._cleanup()

    def _save_multiple_excel_data(self, dfs_dict, institution_input):
        """Save data for multiple institutions into one Excel file with multiple sheets"""
        if not dfs_dict:
            print("No data collected for any institution.")
            return

        try:
            filename = f"college_data_{institution_input['clean_search']}.xlsx"

            with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:
                for inst_name, df in dfs_dict.items():
                    sheet_name = self._clean_filename(inst_name)[:31]  # Excel max sheet name length
                    df.to_excel(writer, sheet_name=sheet_name, index=False)

                    worksheet = writer.sheets[sheet_name]

                    # Freeze header row
                    worksheet.freeze_panes(1, 0)

                    # Set column widths and wrap text for 'Value' column
                    for i, column in enumerate(df.columns):
                        max_len = max(len(str(column)), df[column].astype(str).map(len).max())
                        width = min(max_len + 2, 75)  # Increased limit
                        worksheet.set_column(i, i, width)

                        # If this is the 'Value' column, wrap text
                        if column.lower() == 'value':
                            wrap_format = writer.book.add_format({'text_wrap': True})
                            worksheet.set_column(i, i, width, wrap_format)

                    # Apply bold and background for section headers
                    header_format = writer.book.add_format({
                        'bold': True,
                        'bg_color': '#D9E1F2',  # Light blue
                        'font_size': 12
                    })

                    for row_idx, row in df.iterrows():
                        category = row['Category']
                        if category.startswith('---') or category.upper() == 'GENERAL INFORMATION':
                            worksheet.set_row(row_idx + 1, None, header_format)  # +1 for header row

                    # Optional: add autofilter
                    worksheet.autofilter(0, 0, df.shape[0], df.shape[1] - 1)

            print(f"\nAll data saved to {filename}.")

        except Exception as e:
            print(f"Error saving Excel file: {e}")

    def _expand_all_sections(self):
        """Click the Expand All link if available."""
        try:
            expand_all_link = self.driver.find_element(By.LINK_TEXT, "Expand All")
            expand_all_link.click()
            #print("Clicked Expand All")
            time.sleep(3)
        except Exception as e:
            print(f"Expand All link not found or failed to click: {e}")

    def _setup_driver(self):
        """Set up Chrome driver with proper options"""
        try:
            chrome_options = Options()
            chrome_options.add_argument("--incognito")  # Use incognito mode for a fresh session

            service = Service(self.driver_path)

            # Launch Chrome with specified options
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            print("Browser launched successfully")
        except Exception as e:
            print(f"Failed to set up the browser: {e}")
            raise

    def _get_user_input(self):
        names_input = input("Enter names of Colleges or Universities (comma separated): ")
        names = [name.strip() for name in names_input.split(",")]

        search_term = input(
            "What specific information are you looking for? (e.g., 'Average net price', 'Retention rates'): ")

        search_terms = self._normalize_search_term(search_term)
        keywords = self._extract_keywords(search_terms)

        return {
            "names": names,  # LIST of names now
            "search_term": search_term,
            "search_terms": search_terms,
            "keywords": keywords,
            "clean_search": self._clean_filename(search_term)
        }

    def _search_institution(self, institution_name):
        """Search for an institution on College Navigator"""
        try:
            # Open the website
            self.driver.get(self.base_url)

            # Find and input the institution name
            input_element = self.driver.find_element(By.CLASS_NAME, "instruct")
            input_element.send_keys(institution_name + Keys.ENTER)

            # Add a small delay to allow the page to start loading
            time.sleep(2)

            # Wait for the results table to appear
            WebDriverWait(self.driver, self.wait_time).until(
                EC.presence_of_element_located((By.ID, "ctl00_cphCollegeNavBody_ucResultsMain_tblResults"))
            )

            print("Search results page loaded")
        except TimeoutException:
            print("Timed out waiting for search results to load")
            raise
        except Exception as e:
            print(f"Error searching for institution: {e}")
            raise

    def _select_institution_from_results(self):
        """Select institution from the search results"""
        try:
            # Find all institution links
            results = self.driver.find_elements(By.CSS_SELECTOR, ".resultsTable tr td a[href*='id=']")
            num_results = len(results)

            if num_results == 0:
                print("No institutions found")
                return False
            elif num_results == 1:
                # Select the first institution automatically
                first_result = results[0]
                print(f"Selected: {first_result.text}")

                # Click on the first institution
                first_result.click()
                return True
            else:
                return self._handle_multiple_results(results)
        except Exception as e:
            print(f"Error selecting institution: {e}")
            return False

    def _handle_multiple_results(self, results):
        """Handle case when multiple institutions are found"""
        num_results = len(results)
        print(f"Found {num_results} search results.")
        print(f"Your entry pulled multiple results. Please select one of the following: ")

        for index, result in enumerate(results, start=1):
            print(f"{index}. {result.text}")

        try:
            selection_input = input("Enter the number of the institution you want to select: ")
            user_selection = int(selection_input)

            if 1 <= user_selection <= num_results:
                selected_result = results[user_selection - 1]
                print(f"Selected: {selected_result.text}")
                selected_result.click()
                return True
            else:
                print("Invalid selection.")
                return False
        except ValueError:
            print(f"Enter a valid number.")
            return False

    def _wait_for_page_load(self):
        """Wait for institution page to load after selection"""
        try:
            WebDriverWait(self.driver, self.wait_time).until(
                EC.presence_of_element_located((By.ID, "RightContent"))
            )
            print("Institution page loaded successfully")

            # Wait a bit for the content to load
            time.sleep(2)

            # Wait for the section headers to be present
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".tabtitles"))
            )
        except TimeoutException:
            print("Timed out waiting for institution page to load")
            raise
        except Exception as e:
            print(f"Error waiting for page load: {e}")
            raise

    def _deep_search_in_table(self, table, search_terms, keywords, section_name):
        """
        Performs a thorough search through any type of table structure to find matches
        for search terms or keywords in any cell, row, or column.
        Now also includes header names in row text for better matching (e.g., Bachelor, Master, Doctor).
        """
        try:
            print(f"\nPerforming deep search in table for: {search_terms}")
            found_data = []

            # Extract all rows from the table
            rows = table.find_elements(By.TAG_NAME, "tr")
            if not rows:
                return False

            # Extract header row if present
            headers = []
            header_cells = rows[0].find_elements(By.TAG_NAME, "th")
            if header_cells:
                headers = [cell.text.strip() if cell.text.strip() else f"Column {i + 1}" for i, cell in
                           enumerate(header_cells)]
                print(f"Found headers: {headers}")

            # Process all rows to find relevant data
            for row_idx, row in enumerate(rows):
                # Skip pure header rows
                if row_idx == 0 and header_cells:
                    continue

                # Get all cells in the row
                cells = row.find_elements(By.TAG_NAME, "td")
                if not cells:
                    continue

                # Extract row data
                row_data = [cell.text.strip() for cell in cells]

                # ----------- NEW PART: INCLUDE HEADERS IN ROW TEXT -------------
                row_text_parts = []
                for i, cell_value in enumerate(row_data):
                    header = headers[i] if i < len(headers) else f"Column {i + 1}"
                    row_text_parts.append(f"{header} {cell_value}".lower())
                row_text = " ".join(row_text_parts)
                # -------------------------------------------------------------

                # Check if this row contains any search terms
                term_match = False
                for term in search_terms:
                    if term.lower() in row_text:
                        term_match = True
                        break

                # If no term match, check for keyword match
                keyword_match = False
                if not term_match and keywords:
                    keyword_match = self._keyword_match(row_text, keywords)

                # Process matching rows
                if term_match or keyword_match:
                    category = row_data[0] if row_data else "Unknown"

                    formatted_values = []
                    for i, cell_value in enumerate(row_data[1:], 1):
                        if cell_value and cell_value != "-":
                            if i < len(headers):
                                header = headers[i]
                                formatted_values.append(f"{header}: {cell_value}")
                            else:
                                formatted_values.append(f"Value {i}: {cell_value}")

                    value_str = "; ".join(formatted_values)

                    found_data.append({
                        "Category": category,
                        "Value": value_str,
                        "Section": section_name,
                        "Source": f"Table Search - {'Term Match' if term_match else 'Keyword Match'}"
                    })

                    print(f"Found matching row: {category} with values: {value_str}")

            # If we found data, add it to the all_data collection
            if found_data:
                self.all_data.append({
                    "Category": f"--- {section_name} - SEARCH RESULTS ---",
                    "Value": "",
                    "Section": section_name,
                    "Source": "Deep Table Search"
                })

                for item in found_data:
                    self.all_data.append(item)

                print(f"Added {len(found_data)} items from deep table search")
                return True

            return False

        except Exception as e:
            print(f"Error in deep table search: {e}")
            return False

    def _extract_general_info(self):
        """Extract general institution information from the dashboard section"""
        try:
            # Wait for the dashboard section to be present
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "dashboard"))
            )

            print("\nExtracting general institution information...")

            # Extract institution name and location
            try:
                # Get institution name from headerlg class
                header = self.driver.find_element(By.CLASS_NAME, "headerlg")
                self.institution_info["Institution Name"] = header.text.strip()

                # Get location through JavaScript to extract text node content
                try:
                    # Use JavaScript to get the parent element's text content
                    parent_element = header.find_element(By.XPATH, "./..")
                    full_text = self.driver.execute_script("return arguments[0].textContent;", parent_element)

                    # Remove the institution name to get just the location
                    location_text = full_text.replace(header.text, "").strip()

                    if location_text:
                        self.institution_info["Location"] = location_text
                    else:
                        # Alternative location extraction
                        location_div = self.driver.find_element(By.CLASS_NAME, "dashboard")
                        dashboard_text = location_div.text
                        lines = dashboard_text.split('\n')

                        # Location usually appears right after the institution name
                        if len(lines) > 1:
                            for i, line in enumerate(lines):
                                if header.text in line and i + 1 < len(lines):
                                    self.institution_info["Location"] = lines[i + 1].strip()
                                    break

                        if "Location" not in self.institution_info:
                            self.institution_info["Location"] = "Not found"
                except Exception as loc_error:
                    print(f"Could not extract location: {loc_error}")
                    self.institution_info["Location"] = "Not found"
            except Exception as e:
                print(f"Error extracting institution name: {e}")
                self.institution_info["Institution Name"] = "Not found"
                self.institution_info["Location"] = "Not found"

            # Extract information from the table
            try:
                info_table = self.driver.find_element(By.CLASS_NAME, "layouttab")

                # Print the HTML of the layout table for debugging
                table_html = self.driver.execute_script("return arguments[0].outerHTML;", info_table)
                #print("Found information table with structure:")
                #print(table_html[:200] + "..." if len(table_html) > 200 else table_html)

                rows = info_table.find_elements(By.TAG_NAME, "tr")
                #print(f"Found {len(rows)} rows in information table")

                for row_idx, row in enumerate(rows):
                    try:
                        # Get the label (first cell with class 'srb')
                        label_cells = row.find_elements(By.CLASS_NAME, "srb")
                        if not label_cells:
                            print(f"Row {row_idx + 1} has no label cell with class 'srb', skipping")
                            continue

                        label_cell = label_cells[0]
                        label = label_cell.text.strip().replace(":", "").strip()

                        # Get all cells in the row
                        all_cells = row.find_elements(By.TAG_NAME, "td")
                        if len(all_cells) < 2:
                            print(f"Row {row_idx + 1} has insufficient cells, skipping")
                            continue

                        # Get the value (second cell)
                        value_cell = all_cells[1]  # Second cell (index 1)
                        value = value_cell.text.strip()

                        #print(f"Extracted: {label} = {value}")

                        # Store in the dictionary
                        self.institution_info[label] = value
                    except Exception as e:
                        print(f"Error processing row {row_idx + 1}: {e}")
                        continue

                # Get IPEDS ID and OPE ID
                try:
                    ipeds_elements = self.driver.find_elements(By.CLASS_NAME, "ipeds")
                    if ipeds_elements:
                        ipeds_element = ipeds_elements[0]
                        ipeds_text = ipeds_element.text.strip()
                        #print(f"Found IPEDS information: {ipeds_text}")

                        # Extract IDs using regex
                        ipeds_match = re.search(r"IPEDS ID: (\d+)", ipeds_text)
                        if ipeds_match:
                            self.institution_info["IPEDS ID"] = ipeds_match.group(1)
                            print(f"Extracted IPEDS ID: {ipeds_match.group(1)}")

                        ope_match = re.search(r"OPE ID: (\d+)", ipeds_text)
                        if ope_match:
                            self.institution_info["OPE ID"] = ope_match.group(1)
                            print(f"Extracted OPE ID: {ope_match.group(1)}")
                    else:
                        print("No elements with class 'ipeds' found")
                except Exception as e:
                    print(f"Error extracting IPEDS/OPE IDs: {e}")

            except Exception as e:
                print(f"Error extracting general info table: {e}")

            # Add extracted information to data collection
            self._add_general_info_to_data()

            #print("General institution information extracted successfully")
        except Exception as e:
            print(f"Error extracting general institution information: {e}")

    def _add_general_info_to_data(self):
        """Add general institution information to collected data only if it matches search terms"""
        if not self.institution_info:
            print("No general institution information was collected.")
            return

        matched = False
        for key, value in self.institution_info.items():
            key_lower = key.lower()
            value_lower = str(value).lower()

            # Check if any search term or keyword matches the key or value
            for term in self.search_terms:
                if term.lower() in key_lower or term.lower() in value_lower:
                    matched = True
                    break

        if not matched:
            print("No general information matched the search terms. Skipping adding general info.")
            return  # Don't add general info if nothing matches

        # Otherwise, add general info as before
        self.all_data.append({
            "Category": "GENERAL INFORMATION",
            "Value": "",
            "Section": "General Information",
            "Source": "Institution Dashboard"
        })

        for key, value in self.institution_info.items():
            self.all_data.append({
                "Category": key,
                "Value": value,
                "Section": "General Information",
                "Source": "Institution Dashboard"
            })

        print("General institution information added to data collection (filtered by search terms).")

    def _process_institution_page(self, institution_input):
        """Process the institution page to find and extract data"""
        try:
            # Find all available section headers
            section_headers = self.driver.find_elements(By.CSS_SELECTOR, ".tabtitles")

            # Filter out invalid section headers (non-data categories)
            valid_sections_started = False
            VALID_START_CATEGORY = "GENERAL INFORMATION"

            valid_section_headers = []
            section_titles = []

            # Collect only the real data‐category tabs
            for header in section_headers:
                title = header.text.strip()
                if title == VALID_START_CATEGORY:
                    valid_sections_started = True
                if valid_sections_started:
                    valid_section_headers.append(header)
                    section_titles.append(title)

            # Show them to the user
            print("\nAvailable data categories:")
            for i, title in enumerate(section_titles, 1):
                print(f"{i}. {title}")

            # Always expand all sections and use all of them
            print("Expanding ALL sections (Expand All link)")
            self._expand_all_sections()
            time.sleep(3)  # Give it time to fully expand

            # IMPORTANT CHANGE: ALWAYS search ALL sections for any search term
            # This ensures we don't miss data that might be in unexpected sections
            sections_to_search = list(range(len(section_titles)))
            print(f"Searching across ALL {len(sections_to_search)} sections for '{institution_input['search_term']}'")

            # Now find and process the tables in those sections
            self._find_and_process_tables(institution_input, sections_to_search, section_titles)

        except Exception as e:
            print(f"Error processing institution page: {e}")
            raise

    def _expand_sections(self, section_headers, section_titles, matching_section_indices):
        """Always expand all sections"""
        print("Expanding ALL sections (Expand All link)")
        self._expand_all_sections()
        return list(range(len(section_headers)))

    def _click_expand_all(self):
        """Click the Expand All link"""
        try:
            expand_all_link = self.driver.find_element(By.LINK_TEXT, "Expand All")
            expand_all_link.click()
            #print("Clicked Expand All")
        except Exception as e:
            print(f"Expand All not found or failed to click: {e}")

    def _handle_no_matches(self, all_tables, graph_tables, institution_input, section_titles, sections_to_search):
        """Handle the case when no matches are found"""
        print(f"\nNo tables found containing '{institution_input['search_term']}' or related terms")

        # Try enhanced search across all tables first
        print("\nAttempting enhanced search across all tables...")
        found_any = False

        for i, table in enumerate(all_tables):
            try:
                # Try to determine section name
                section_name = "Unknown Section"
                for section_idx in sections_to_search:
                    if section_idx < len(section_titles):
                        # Simple text-based matching to find section
                        section_title = section_titles[section_idx]
                        table_html = self.driver.execute_script("return arguments[0].outerHTML;", table)
                        if section_title.lower() in table_html.lower():
                            section_name = section_title
                            break

                # Perform enhanced search in this table
                if self._enhanced_table_search(table, institution_input["search_terms"], section_name):
                    found_any = True
            except Exception as e:
                print(f"Error during enhanced search of table {i}: {e}")
                continue

        # If enhanced search didn't find anything, offer to search more broadly
        if not found_any:
            search_broadly = input("\nWould you like to search more broadly using just keywords? (y/n): ").lower()

            if search_broadly == 'y':
                # Use a more general keyword search across all tables
                print("\nSearching all tables with individual keywords...")

                # Search for keyword matches
                keyword_matches = self._find_keyword_matches(
                    all_tables,
                    graph_tables,
                    institution_input["keywords"],
                    section_titles,
                    sections_to_search
                )

                # Process keyword matches if found
                if keyword_matches:
                    self._process_keyword_matches(keyword_matches, institution_input)
                else:
                    print("No potential matches found with keyword search.")

    def _enhanced_table_search(self, table, search_terms, section_name):
        """Performs a deep search in a table for specific terms regardless of column structure"""
        try:
            print(f"\nPerforming enhanced search in table for terms: {search_terms}")
            found_items = []

            # Extract all rows
            rows = table.find_elements(By.TAG_NAME, "tr")

            # First identify headers if present
            headers = []
            for row in rows:
                th_cells = row.find_elements(By.TAG_NAME, "th")
                if th_cells:
                    headers.extend([cell.text.strip() for cell in th_cells])

            print(f"Found headers: {headers}")

            # Search through all rows for search terms
            for row in rows:
                cells = row.find_elements(By.TAG_NAME, "td")
                if not cells:
                    continue

                row_data = [cell.text.strip() for cell in cells]
                row_text = " ".join(row_data).lower()

                # Check if any search term is found in this row
                for term in search_terms:
                    if term.lower() in row_text:
                        print(f"Found match in row: {row_data}")

                        # Handle the first cell typically containing the category/name
                        category = row_data[0] if row_data else "Unknown"

                        # Create a value string with all non-empty data
                        values = []
                        for i, cell_text in enumerate(row_data[1:], 1):
                            if cell_text and cell_text != "-":
                                # Try to associate with a header if possible
                                if i < len(headers):
                                    values.append(f"{headers[i]}: {cell_text}")
                                else:
                                    values.append(f"Column {i}: {cell_text}")

                        value_str = "; ".join(values)

                        found_items.append({
                            "Category": category,
                            "Value": value_str,
                            "Section": section_name,
                            "Source": "Enhanced Table Search"
                        })

                        break  # Found a match in this row, move to next

            # Add results to the data collection
            if found_items:
                # Add a section header
                self.all_data.append({
                    "Category": f"--- {section_name} - SEARCH RESULTS ---",
                    "Value": "",
                    "Section": section_name,
                    "Source": "Enhanced Table Search"
                })

                # Add found items
                for item in found_items:
                    self.all_data.append(item)

                print(f"Added {len(found_items)} items from enhanced search")
                return True

            return False

        except Exception as e:
            print(f"Error in enhanced table search: {e}")
            return False

    def _expand_section(self, section_element):
        """Expand a specific section by clicking its header"""
        try:
            # Scroll to make element visible
            self.driver.execute_script("arguments[0].scrollIntoView(true);", section_element)
            time.sleep(0.5)  # Small delay to ensure element is properly in view

            # Click to expand
            section_element.click()
            time.sleep(1)  # Wait for expansion animation

            return True
        except Exception as e:
            print(f"Failed to expand section: {e}")
            return False

    def _find_and_process_tables(self, institution_input, sections_to_search, section_titles):
        """Find and process tables that match the search criteria"""
        # First, check if the search term is already in the general information
        search_in_general_info = self._check_general_info_for_search(
            institution_input["search_terms"],
            institution_input["keywords"]
        )

        if search_in_general_info:
            print(f"\nInformation found in General Information section: {search_in_general_info}")
            return

        # Find all tables
        #all_tables = self.driver.find_elements(By.CLASS_NAME, "tabular")
        all_tables = self.driver.find_elements(By.TAG_NAME, "table")
        #print(f"Found {len(all_tables)} tables with class 'tabular'")

        # new: grab both the <table class="graphtabs"> graphs, and any inline SVGs in tabcontent
        graph_tables = self.driver.find_elements(
            By.CSS_SELECTOR,
            ".graphtabs, .tabcontent svg"
        )

        #print(f"Found {len(graph_tables)} tables with class 'graphtabs'")

        # Find matches in regular tables
        matching_tables = self._find_matching_tables(
            all_tables,
            institution_input["search_terms"],
            institution_input["keywords"],
            section_titles,
            sections_to_search
        )

        # Find matches in graph tables
        matching_graph_tables = self._find_matching_graph_tables(
            graph_tables,
            institution_input["search_terms"],
            institution_input["keywords"],
            section_titles,
            sections_to_search
        )

        # Process matching tables if found
        if matching_tables or matching_graph_tables:
            self._process_matching_tables(
                matching_tables,
                matching_graph_tables,
                institution_input
            )
        else:
            # No matches found with standard approach, use enhanced search
            self._handle_no_matches(
                all_tables,
                graph_tables,
                institution_input,
                section_titles,
                sections_to_search
            )

    def _find_matching_tables(self, tables, search_terms, keywords, section_titles, sections_to_search):
        """Find regular tables that match the search criteria"""
        matching_tables = []

        # Try to determine which section each table belongs to
        for i, table in enumerate(tables):
            # Get parent elements to try to determine the section
            parent_elements = []
            current_element = table
            section_name = "Unknown Section"

            print(f"\nDebugging section detection for table {i}:")

            # Try to find the containing section by traversing up the DOM
            for level in range(5):  # Only go up a few levels to avoid traversing too far
                try:
                    parent = current_element.find_element(By.XPATH, "..")
                    parent_elements.append(parent)
                    current_element = parent

                    # Get a preview of parent text for debugging
                    parent_text = parent.text[:100] + "..." if len(parent.text) > 100 else parent.text
                    print(f"  Level {level + 1} parent text: {parent_text}")

                    # Try to find a section title nearby
                    for section_idx in sections_to_search:
                        section_title = section_titles[section_idx]
                        print(f"    Checking if '{section_title}' is in parent text...")
                        if section_title.lower() in parent.text.lower():
                            section_name = section_title
                            print(f"    MATCH FOUND! Setting section to '{section_name}'")
                            break

                    # If we found a section, no need to go up further
                    if section_name != "Unknown Section":
                        print(f"  Found section '{section_name}' at level {level + 1}")
                        break

                except Exception as e:
                    print(f"  Error at level {level + 1}: {e}")
                    break

            print(f"  Final section determination for table {i}: '{section_name}'")

            table_text = table.text.lower()

            # Check each search term
            found_match = False
            for term in search_terms:
                if term.lower() in table_text:
                    matching_tables.append((i, table, "exact", section_name))
                    print(f"\nFound exact match in table {i} in section '{section_name}'")
                    found_match = True
                    break

            # If no exact match, try keyword matching
            if not found_match and keywords:
                if self._keyword_match(table_text, keywords):
                    matching_tables.append((i, table, "keyword", section_name))
                    print(f"\nFound keyword match in table {i} in section '{section_name}'")

        return matching_tables

    def _find_matching_graph_tables(self, graph_containers, search_terms, keywords, section_titles, sections_to_search):
        matching = []

        # Check if we're searching for retention-related content
        retention_search = any('retention' in term.lower() for term in search_terms)

        for idx, cont in enumerate(graph_containers):
            # ——— 1) Determine the section this container belongs to ———
            section_name = "Unknown Section"
            parent = cont
            for _ in range(5):
                try:
                    parent = parent.find_element(By.XPATH, "..")
                    txt = parent.text.lower()
                    for si in sections_to_search:
                        if section_titles[si].lower() in txt:
                            section_name = section_titles[si]
                            break
                    if section_name != "Unknown Section":
                        break
                except:
                    break

            # ——— 2) Enhanced title detection ———
            title_text = ""

            # a) Method: thead → th
            try:
                title_text = cont.find_element(By.CSS_SELECTOR, "thead th").text.strip().lower()
            except:
                pass

            # b) Method: <svg><title>
            if not title_text:
                try:
                    title_text = cont.find_element(By.XPATH, ".//svg/title").get_attribute(
                        "textContent").strip().lower()
                except:
                    pass

            # c) Method: <caption>
            if not title_text:
                try:
                    title_text = cont.find_element(By.TAG_NAME, "caption").text.strip().lower()
                except:
                    pass

            # d) Method: Highcharts title
            if not title_text:
                try:
                    title_text = cont.find_element(By.CSS_SELECTOR, ".highcharts-title tspan").text.strip().lower()
                except:
                    pass

            # e) Method: Check for preceding div.tablenames
            if not title_text:
                try:
                    title_divs = cont.find_elements(By.XPATH, "preceding-sibling::div[contains(@class,'tablenames')]")
                    if title_divs:
                        title_text = title_divs[0].text.strip().lower()
                except:
                    pass

            # f) Method: Check for any SVG text node containing our terms
            if not title_text:
                for term in search_terms:
                    try:
                        text_elems = cont.find_elements(By.XPATH, ".//svg//*[local-name()='text']")
                        for txt_el in text_elems:
                            if term.lower() in txt_el.text.strip().lower():
                                title_text = txt_el.text.strip().lower()
                                break
                        if title_text:
                            break
                    except:
                        pass

            # g) Method: Check image alt text
            alt_text = ""
            try:
                for img in cont.find_elements(By.TAG_NAME, "img"):
                    img_alt = (img.get_attribute("alt") or "").strip().lower()
                    if any(term.lower() in img_alt for term in search_terms):
                        alt_text = img_alt
                        break
            except:
                pass

            # ——— 3) Special handling for retention data ———
            if retention_search:
                # For retention searches, check for retention in the full container text
                container_text = cont.text.strip().lower()

                # Check if this is in SERVICEMEMBERS AND VETERANS section
                if "SERVICEMEMBERS AND VETERANS" in section_name.upper():
                    print(f"Examining container {idx} in SERVICEMEMBERS section")

                    # Look for retention or education benefit keywords
                    if "retention" in container_text or "benefit" in container_text:
                        print(f"→ Found potential veterans retention data in container {idx}")
                        matching.append((idx, cont, "veterans_retention_match", section_name))
                        continue

                # Check if this has retention in title or alt text
                if "retention" in title_text or "retention" in alt_text:
                    print(f"→ Found retention match in container {idx} ('{title_text[:40]}…') in '{section_name}'")
                    matching.append((idx, cont, "retention_match", section_name))
                    continue

            # ——— 4) Standard search term matching ———
            # If any title or alt text mentions our term, we've got a match
            if (title_text and any(term.lower() in title_text for term in search_terms)) or \
                    (alt_text and any(term.lower() in alt_text for term in search_terms)):
                matching.append((idx, cont, "title_match", section_name))
                print(f"→ TITLE match on graph {idx} ('{title_text[:40]}…') in '{section_name}'")
                continue

            # ——— 5) Keyword matching as fallback ———
            if keywords:
                # Check if all keywords appear in the container text
                container_text = cont.text.strip().lower()
                if all(keyword.lower() in container_text for keyword in keywords):
                    matching.append((idx, cont, "keyword_match", section_name))
                    print(f"→ KEYWORD match on graph {idx} in '{section_name}'")
                    continue

        return matching

    def _try_alternative_section_detection(self, table, section_titles):
        """Try alternative methods to detect which section a table belongs to"""
        print("\nTrying alternative section detection methods:")

        # Method 1: Look for closest preceding section header
        try:
            # Get table position
            table_position = self.driver.execute_script(
                "return arguments[0].getBoundingClientRect().top;", table)
            print(f"  Table position (top): {table_position}")

            # Get all section headers
            section_elements = self.driver.find_elements(By.CSS_SELECTOR, ".tabtitles")
            closest_section = None
            closest_distance = float('inf')

            print("  Checking section headers by position:")
            for idx, section_elem in enumerate(section_elements):
                # Get section position
                section_position = self.driver.execute_script(
                    "return arguments[0].getBoundingClientRect().top;", section_elem)
                distance = table_position - section_position

                section_title = section_elem.text.strip()
                print(f"    Section '{section_title}' position: {section_position}, distance: {distance}")

                # Only consider sections that come before the table (positive distance)
                if distance > 0 and distance < closest_distance:
                    closest_distance = distance
                    closest_section = section_elem
                    print(f"      (New closest section: '{section_title}' with distance {distance})")

            if closest_section:
                section_name = closest_section.text.strip()
                print(f"  Found closest section by position: '{section_name}'")
                return section_name
            else:
                print("  No valid section found by position")

        except Exception as e:
            print(f"  Error in position-based detection: {e}")

        # Method 2: Try XPath-based approaches
        try:
            print("  Trying XPath-based approaches:")

            # Try to find the section by looking for parent tabcontent div
            try:
                # Look for parent div with tabcontent class
                tabcontent = table.find_element(By.XPATH, "./ancestor::div[contains(@class, 'tabcontent')]")
                print("    Found tabcontent container")

                # Find the associated header
                header = tabcontent.find_element(By.XPATH, "preceding-sibling::div[contains(@class, 'tabtitles')][1]")
                section_name = header.text.strip()
                print(f"    Found section header via XPath: '{section_name}'")
                return section_name
            except Exception as e:
                print(f"    Error finding tabcontent container: {e}")

            # Try another approach - look at HTML attributes or IDs
            try:
                # Try to get ID of parent elements
                current = table
                for level in range(3):
                    try:
                        parent = current.find_element(By.XPATH, "..")
                        parent_id = parent.get_attribute("id")
                        parent_class = parent.get_attribute("class")
                        print(f"    Level {level + 1} parent - ID: '{parent_id}', Class: '{parent_class}'")

                        # Check if ID or class contains section info
                        if parent_id and any(
                                title.lower().replace(' ', '') in parent_id.lower() for title in section_titles):
                            matched_title = next(title for title in section_titles if
                                                 title.lower().replace(' ', '') in parent_id.lower())
                            print(f"    Found match in parent ID: '{matched_title}'")
                            return matched_title

                        current = parent
                    except Exception as nested_e:
                        print(f"    Error at level {level + 1}: {nested_e}")
                        break
            except Exception as e:
                print(f"    Error in ID-based detection: {e}")

        except Exception as e:
            print(f"  Error in XPath approaches: {e}")

        print("  All alternative detection methods failed")
        return "Unknown Section"

    def _process_matching_tables(self, matching_tables, matching_graph_tables, institution_input):
        """Process tables that match the search criteria"""
        print(
            f"\nFound {len(matching_tables)} matching regular tables and {len(matching_graph_tables)} matching graph tables")

        # Track if we've found any data
        found_any_data = False

        # Process matching regular tables
        for i, table, match_type, section_name in matching_tables:
            print(f"\nProcessing regular table {i} (match type: {match_type}) in section '{section_name}':")

            # First try deep search approach for more intelligent extraction
            if self._deep_search_in_table(
                    table,
                    institution_input["search_terms"],
                    institution_input["keywords"],
                    section_name
            ):
                found_any_data = True
            else:
                # Fall back to the original method if deep search doesn't find anything
                self._process_regular_table(
                    table,
                    i,
                    section_name,
                    institution_input["search_terms"],
                    institution_input["keywords"]
                )
                found_any_data = True

        # Process matching graph tables
        for i, table, match_type, section_name in matching_graph_tables:
            print(f"\nProcessing graph table {i} (match type: {match_type}) in section '{section_name}':")
            self._process_graph_table(
                table,
                i,
                section_name
            )
            found_any_data = True

        # If we didn't find any useful data in the matching tables, try a deeper search
        if not found_any_data:
            print("\nNo useful data found in matching tables. Attempting deeper search...")
            self._handle_no_matches(
                [table for _, table, _, _ in matching_tables],
                [table for _, table, _, _ in matching_graph_tables],
                institution_input,
                [],  # We don't need section titles here
                []  # We don't need section indices here
            )

    def _handle_no_matches(self, all_tables, graph_tables, institution_input, section_titles, sections_to_search):
        """Handle the case when no matches are found"""
        print(f"\nNo tables found containing '{institution_input['search_term']}' or related terms")

        # Try deep search across all tables
        print("\nAttempting deep search across all tables...")
        found_any = False

        # First check regular tables
        for i, table in enumerate(all_tables):
            try:
                # Determine section name if possible
                section_name = "Unknown Section"
                if section_titles:
                    for section_idx in sections_to_search:
                        if section_idx < len(section_titles):
                            section_title = section_titles[section_idx]
                            table_html = self.driver.execute_script("return arguments[0].outerHTML;", table)
                            if section_title.lower() in table_html.lower():
                                section_name = section_title
                                break

                # Perform deep search
                if self._deep_search_in_table(
                        table,
                        institution_input["search_terms"],
                        institution_input["keywords"],
                        section_name
                ):
                    found_any = True
            except Exception as e:
                print(f"Error during deep search of table {i}: {e}")
                continue

        # Check graph tables too
        for i, table in enumerate(graph_tables):
            try:
                # Determine section name if possible
                section_name = "Unknown Section"
                if section_titles:
                    for section_idx in sections_to_search:
                        if section_idx < len(section_titles):
                            section_title = section_titles[section_idx]
                            table_html = self.driver.execute_script("return arguments[0].outerHTML;", table)
                            if section_title.lower() in table_html.lower():
                                section_name = section_title
                                break

                # Process graph table directly since they have a special structure
                self._process_graph_table(table, i, section_name)
                found_any = True
            except Exception as e:
                print(f"Error processing graph table {i}: {e}")
                continue

        # If deep search didn't find anything, offer to search more broadly
        if not found_any:
            search_broadly = input("\nWould you like to search more broadly using just keywords? (y/n): ").lower()

            if search_broadly == 'y':
                # Use a more general keyword search across all tables
                print("\nSearching all tables with individual keywords...")

                # Search for keyword matches
                keyword_matches = self._find_keyword_matches(
                    all_tables,
                    graph_tables,
                    institution_input["keywords"],
                    section_titles,
                    sections_to_search
                )

                # Process keyword matches if found
                if keyword_matches:
                    self._process_keyword_matches(keyword_matches, institution_input)
                else:
                    print("No potential matches found with keyword search.")

    def _find_keyword_matches(self, regular_tables, graph_tables, keywords, section_titles, sections_to_search):
        """Find tables that match individual keywords"""
        # Track matches
        keyword_matches = []

        # Search regular tables
        for i, table in enumerate(regular_tables):
            # Get parent elements to try to determine the section
            parent_elements = []
            current_element = table
            section_name = "Unknown Section"

            # Try to find the containing section by traversing up the DOM
            for _ in range(5):  # Only go up a few levels to avoid traversing too far
                try:
                    parent = current_element.find_element(By.XPATH, "..")
                    parent_elements.append(parent)
                    current_element = parent

                    # Try to find a section title nearby
                    for section_idx in sections_to_search:
                        section_title = section_titles[section_idx]
                        if section_title.lower() in parent.text.lower():
                            section_name = section_title
                            break
                except:
                    break

            table_text = table.text.lower()
            matched_keywords = []

            # Check each keyword individually
            for keyword in keywords:
                if keyword.lower() in table_text:
                    matched_keywords.append(keyword)

            if matched_keywords:
                match_score = len(matched_keywords) / len(keywords) if keywords else 0
                keyword_matches.append(("regular", i, table, matched_keywords, match_score, section_name))

        # Search graph tables
        for i, table in enumerate(graph_tables):
            # Get parent elements to try to determine the section
            parent_elements = []
            current_element = table
            section_name = "Unknown Section"

            # Try to find the containing section by traversing up the DOM
            for _ in range(5):  # Only go up a few levels to avoid traversing too far
                try:
                    parent = current_element.find_element(By.XPATH, "..")
                    parent_elements.append(parent)
                    current_element = parent

                    # Try to find a section title nearby
                    for section_idx in sections_to_search:
                        section_title = section_titles[section_idx]
                        if section_title.lower() in parent.text.lower():
                            section_name = section_title
                            break
                except:
                    break

            table_text = table.text.lower()
            matched_keywords = []

            # Check each keyword in the visible text
            for keyword in keywords:
                if keyword.lower() in table_text:
                    matched_keywords.append(keyword)

            # Also check image alt text
            images = table.find_elements(By.TAG_NAME, "img")
            for img in images:
                alt_text = img.get_attribute("alt").lower()
                for keyword in keywords:
                    if keyword.lower() in alt_text and keyword not in matched_keywords:
                        matched_keywords.append(keyword)

            if matched_keywords:
                match_score = len(matched_keywords) / len(keywords) if keywords else 0
                keyword_matches.append(("graph", i, table, matched_keywords, match_score, section_name))

        # Sort matches by score (highest first)
        keyword_matches.sort(key=lambda x: x[4], reverse=True)

        return keyword_matches

    def _process_keyword_matches(self, keyword_matches, institution_input):
        """Process tables that match individual keywords"""
        print(f"\nFound {len(keyword_matches)} potential matches using keyword search")

        # Print top matches
        for j, (table_type, i, table, matched_kw, score, section_name) in enumerate(keyword_matches[:5]):
            print(f"\nMatch {j + 1} ({table_type} table {i}, score: {score:.2f}) in section '{section_name}':")
            print(f"Matched keywords: {', '.join(matched_kw)}")

            # If it's a graph table, extract data
            if table_type == "graph":
                graph_data = self._extract_graph_data(table)
                if graph_data:
                    print("Graph title:", graph_data.get("title", "Unknown"))

                    # Extract data from images
                    if "images" in graph_data:
                        for img_data in graph_data["images"]:
                            if "alt" in img_data:
                                print(f"Alt text: {img_data['alt']}")
                            if "extracted_data" in img_data:
                                for label, value in img_data["extracted_data"].items():
                                    print(f"  {label}: {value}%")
            else:
                # Print a preview of the table text
                preview = table.text[:200] + "..." if len(table.text) > 200 else table.text
                print(f"Table preview: {preview}")

        # Ask if user wants to process any of these matches
        process_match = input("\nWould you like to process any of these matches? Enter number or 'n': ")

        if process_match.lower() != 'n':
            try:
                match_idx = int(process_match) - 1
                if 0 <= match_idx < len(keyword_matches):
                    table_type, i, table, matched_kw, score, section_name = keyword_matches[match_idx]

                    print(f"\nProcessing {table_type} table {i} in section '{section_name}':")

                    if table_type == "graph":
                        self._process_keyword_graph_table(
                            table,
                            i,
                            section_name
                        )
                    else:
                        self._process_keyword_regular_table(
                            table,
                            i,
                            section_name
                        )
            except ValueError:
                print("Invalid selection. No match processed.")

    def _process_regular_table(self, table, index, section_name, search_terms, keywords):
        """Process a regular tabular data table"""
        try:
            # Extract table data
            rows = table.find_elements(By.TAG_NAME, "tr")
            headers = []
            data = []

            # Add a section header to the data
            self.all_data.append({
                "Category": f"--- {section_name} - TABLE {index} ---",
                "Value": "",
                "Section": section_name,
                "Source": f"Table {index}"
            })

            for row_idx, row in enumerate(rows):
                # Get all cells (th for headers, td for data)
                header_cells = row.find_elements(By.TAG_NAME, "th")
                data_cells = row.find_elements(By.TAG_NAME, "td")

                # Process header row
                if header_cells and row_idx == 0:
                    headers = [cell.text for cell in header_cells]
                    print(f"Headers: {headers}")

                # Process data row
                if data_cells:
                    row_data = [cell.text for cell in data_cells]
                    data.append(row_data)
                    # Only print first few rows if there are many
                    if row_idx < 5:
                        print(f"Row data: {row_data}")
                    elif row_idx == 5:
                        print("...")

            # Create DataFrame if we have data
            if headers and data:
                self._add_table_to_data(headers, data, section_name, search_terms, keywords, f"Table {index}")

        except Exception as e:
            print(f"Error processing regular table {index}: {e}")

    def _add_table_to_data(self, headers, data, section_name, search_terms, keywords, source):
        """Add table data to consolidated data collection"""
        try:
            # Handle special case where first header is empty (row labels)
            if headers[0] == '':
                for row in data:
                    if not row:  # Skip empty rows
                        continue

                    # If first cell contains label
                    label = row.pop(0) if row else "Unknown"

                    if not row:  # Skip if no data after removing label
                        continue

                    # Create a string of all values
                    value_str = "; ".join([str(val) for val in row])

                    # Add to all_data
                    self.all_data.append({
                        "Category": label,
                        "Value": value_str,
                        "Section": section_name,
                        "Source": source
                    })

                    # Check if this matches search terms
                    for term in search_terms:
                        if term.lower() in label.lower():
                            print(f"\nFound specific data match: '{label}'")
                            break
            else:
                for row in data:
                    # For tables with headers, use first column as category and join the rest
                    if len(row) > 0:
                        category = row[0]

                        # If there are more columns, combine them
                        if len(row) > 1:
                            values = [f"{headers[i]}: {row[i]}" for i in range(1, len(row))]
                            value_str = "; ".join(values)
                        else:
                            value_str = ""

                        # Add to all_data
                        self.all_data.append({
                            "Category": category,
                            "Value": value_str,
                            "Section": section_name,
                            "Source": source
                        })

                        # Check if the row contains search terms
                        row_text = ' '.join(row).lower()
                        for term in search_terms:
                            if term.lower() in row_text:
                                print(f"\nFound specific data match in row: '{category}'")
                                break

            #print(f"Table data added to consolidated data")
        except Exception as e:
            print(f"Error adding table to data: {e}")

    def _process_graph_table(self, table, index, section_name):
        """Process a graph table with visual data"""
        try:
            # Extract data from the graph table
            graph_data = self._extract_graph_data(table)

            if graph_data:
                print("Extracted graph data:")

                # Add a section header to the data
                self.all_data.append({
                    "Category": f"--- {section_name} - GRAPH {index} ---",
                    "Value": "",
                    "Section": section_name,
                    "Source": f"Graph {index}"
                })

                # Add title if available
                if "title" in graph_data:
                    title = graph_data["title"]
                    print(f"Title: {title}")
                    self.all_data.append({
                        "Category": "Graph Title",
                        "Value": title,
                        "Section": section_name,
                        "Source": f"Graph {index}"
                    })

                # Add data extracted from images
                if "images" in graph_data:
                    for j, img_data in enumerate(graph_data["images"]):
                        print(f"\nImage {j + 1}:")

                        if "alt" in img_data:
                            alt_text = img_data["alt"]
                            print(f"Alt text: {alt_text}")
                            self.all_data.append({
                                "Category": f"Image {j + 1} Description",
                                "Value": alt_text,
                                "Section": section_name,
                                "Source": f"Graph {index}"
                            })

                        if "extracted_data" in img_data:
                            print("Data extracted from alt text:")
                            for label, value in img_data["extracted_data"].items():
                                # Add % if not already present
                                if not str(value).endswith('%'):
                                    value = f"{value}%"

                                print(f"  {label}: {value}")

                                # Create a descriptive category name based on section and label
                                category = label

                                # For SERVICEMEMBERS section, make it clear this is for benefit users
                                if "SERVICEMEMBERS AND VETERANS" in section_name.upper() and "benefit" not in label.lower():
                                    category = f"Education Benefit Users - {label}"

                                self.all_data.append({
                                    "Category": category,
                                    "Value": value,
                                    "Section": section_name,
                                    "Source": f"Graph {index} - Image {j + 1}"
                                })

                # Add any text data found in the table
                if "text_data" in graph_data:
                    print("\nText data from table:")
                    for label, value in graph_data["text_data"].items():
                        print(f"  {label}: {value}")
                        self.all_data.append({
                            "Category": label,
                            "Value": value,
                            "Section": section_name,
                            "Source": f"Graph {index} - Table Text"
                        })

                print("Graph data added to consolidated data")
            else:
                print("Could not extract structured data from this graph table")
        except Exception as e:
            print(f"Error processing graph table {index}: {e}")

    def _process_keyword_regular_table(self, table, index, section_name):
        """Process a regular table found through keyword search"""
        try:
            # Extract table data
            rows = table.find_elements(By.TAG_NAME, "tr")
            headers = []
            data = []

            # Add a section header to the data
            self.all_data.append({
                "Category": f"--- {section_name} - KEYWORD MATCH TABLE {index} ---",
                "Value": "",
                "Section": section_name,
                "Source": f"Keyword Match Table {index}"
            })

            for row_idx, row in enumerate(rows):
                # Get all cells (th for headers, td for data)
                header_cells = row.find_elements(By.TAG_NAME, "th")
                data_cells = row.find_elements(By.TAG_NAME, "td")

                # Process header row
                if header_cells and row_idx == 0:
                    headers = [cell.text for cell in header_cells]
                    print(f"Headers: {headers}")

                # Process data row
                if data_cells:
                    row_data = [cell.text for cell in data_cells]
                    data.append(row_data)
                    # Only print first few rows
                    if row_idx < 3:
                        print(f"Row data: {row_data}")

            # Create DataFrame if we have data
            if headers and data:
                # Use empty lists for search_terms and keywords since we don't need to highlight matches
                self._add_table_to_data(headers, data, section_name, [], [], f"Keyword Match Table {index}")
                print(f"Regular table data added to consolidated data")
        except Exception as e:
            print(f"Error processing regular table: {e}")

    def _process_keyword_graph_table(self, table, index, section_name):
        """Process a graph table found through keyword search"""
        try:
            # Extract data from graph table
            graph_data = self._extract_graph_data(table)

            if graph_data:
                # Add a section header to the data
                self.all_data.append({
                    "Category": f"--- {section_name} - KEYWORD MATCH GRAPH {index} ---",
                    "Value": "",
                    "Section": section_name,
                    "Source": f"Keyword Match Graph {index}"
                })

                # Add title if available
                if "title" in graph_data:
                    title = graph_data["title"]
                    self.all_data.append({
                        "Category": "Graph Title",
                        "Value": title,
                        "Section": section_name,
                        "Source": f"Keyword Match Graph {index}"
                    })

                # Add data extracted from images
                if "images" in graph_data:
                    for j, img_data in enumerate(graph_data["images"]):
                        if "alt" in img_data:
                            alt_text = img_data["alt"]
                            self.all_data.append({
                                "Category": f"Image {j + 1} Description",
                                "Value": alt_text,
                                "Section": section_name,
                                "Source": f"Keyword Match Graph {index}"
                            })

                        if "extracted_data" in img_data:
                            for label, value in img_data["extracted_data"].items():
                                self.all_data.append({
                                    "Category": label,
                                    "Value": f"{value}%",
                                    "Section": section_name,
                                    "Source": f"Keyword Match Graph {index} - Image {j + 1}"
                                })

                # Add any text data found in the table
                if "text_data" in graph_data:
                    for label, value in graph_data["text_data"].items():
                        self.all_data.append({
                            "Category": label,
                            "Value": value,
                            "Section": section_name,
                            "Source": f"Keyword Match Graph {index} - Table Text"
                        })

                print(f"Graph data added to consolidated data")
            else:
                print("Could not extract structured data from this graph table")
        except Exception as e:
            print(f"Error processing graph table: {e}")

    def _extract_graph_data(self, table):
        """Extract data from graph tables and image elements containing percentage data"""
        try:
            # Support various graph containers (tables, images, SVG elements)
            table_class = table.get_attribute("class") or ""
            tag_name = table.tag_name.lower()

            # Store all extracted data
            graph_data = {}

            # ---- TITLE EXTRACTION (try multiple methods) ----
            title_candidates = []

            # Method 1: From thead/th
            try:
                thead = table.find_element(By.TAG_NAME, "thead")
                title_elem = thead.find_element(By.TAG_NAME, "th")
                title_candidates.append(title_elem.text.strip())
            except:
                pass

            # Method 2: From preceding div.tablenames
            try:
                parent = table
                for i in range(3):  # Try a few levels up
                    try:
                        parent = parent.find_element(By.XPATH, "./..")
                        # Try to find closest tablenames div
                        title_divs = parent.find_elements(By.XPATH, ".//div[contains(@class,'tablenames')]")
                        for div in title_divs:
                            title_candidates.append(div.text.strip())
                    except:
                        break
            except:
                pass

            # Method 3: From caption or title elements
            for elem_type in ["caption", "title"]:
                try:
                    elem = table.find_element(By.TAG_NAME, elem_type)
                    if elem.tag_name == "title":  # SVG title
                        title_candidates.append(elem.get_attribute("textContent").strip())
                    else:
                        title_candidates.append(elem.text.strip())
                except:
                    pass

            # Method 4: From any heading nearby
            try:
                for h_level in range(1, 6):
                    headings = table.find_elements(By.XPATH, f"preceding-sibling::h{h_level}")
                    for heading in headings:
                        title_candidates.append(heading.text.strip())
            except:
                pass

            # Select the best non-empty title
            title_candidates = [t for t in title_candidates if t]
            if title_candidates:
                graph_data["title"] = title_candidates[0]
            else:
                graph_data["title"] = "Unknown Graph"

            # ---- IMAGE DATA EXTRACTION ----
            # Look for image tags that contain graph data
            graph_images = table.find_elements(By.TAG_NAME, "img")

            # If we found graph images, extract data from their attributes
            image_data = []
            for img in graph_images:
                img_data = {}
                img_data["alt"] = img.get_attribute("alt") or ""
                img_data["src"] = img.get_attribute("src") or ""

                print(f"Examining image with alt text: {img_data['alt']}")

                # Enhanced pattern matching for extracting data from alt text
                # More comprehensive patterns to catch various formats
                data_patterns = [
                    r'([\w\s-]+):\s+(\d+\.?\d*)%',  # Basic format: "Label: 96%"
                    r'([\w\s-]+)\s+students?:\s+(\d+\.?\d*)%',  # Student format: "Full-time students: 84%"
                    r'([\w\s-]+)\s+rate:\s+(\d+\.?\d*)%',  # Rate format: "Retention rate: 96%"
                    r'(\d+\.?\d*)%\s+of\s+([\w\s-]+)',  # Reverse format: "96% of full-time students"
                    r'(\d+\.?\d*)%\s+([\w\s-]+)'  # Simple reverse: "96% students"
                ]

                extracted_data = {}
                for pattern in data_patterns:
                    matches = re.findall(pattern, img_data["alt"])
                    for match in matches:
                        # Handle different pattern formats (label-value vs value-label)
                        if match[0].isdigit() or match[0].replace('.', '', 1).isdigit():
                            value, label = match
                        else:
                            label, value = match

                        extracted_data[label.strip()] = value

                if extracted_data:
                    img_data["extracted_data"] = extracted_data
                    print(f"Extracted data from image: {extracted_data}")

                # Special handling for EDUCATION BENEFIT USERS retention rates
                # Look specifically for education benefit patterns in alt text
                if "BENEFIT" in img_data["alt"].upper() and "RETENTION" in img_data["alt"].upper():
                    print("Found education benefit users retention rate graph!")

                    # Try to extract the percentage directly
                    benefit_patterns = [
                        r'Full-time students:\s+(\d+)%',
                        r'(\d+)%\s+of\s+full-time students',
                        r'retention rate.*?(\d+)%',
                        r'(\d+)%.*?retention rate'
                    ]

                    for pattern in benefit_patterns:
                        benefit_match = re.search(pattern, img_data["alt"], re.IGNORECASE)
                        if benefit_match:
                            rate_value = benefit_match.group(1)
                            if "extracted_data" not in img_data:
                                img_data["extracted_data"] = {}
                            img_data["extracted_data"]["Education benefit users"] = rate_value
                            print(f"Extracted education benefit retention rate: {rate_value}%")
                            break

                # Extract data from image URL parameters
                try:
                    src_url = img_data["src"]
                    if "Graph.aspx" in src_url:
                        # Parse URL parameters
                        query_params = {}
                        if "?" in src_url:
                            query_string = src_url.split("?")[1]
                            params = query_string.split("&")
                            for param in params:
                                if "=" in param:
                                    key, value = param.split("=", 1)
                                    query_params[key] = urllib.parse.unquote(value)

                        # Extract data parameters
                        if "data" in query_params:
                            img_data["url_data"] = query_params["data"]
                        if "label" in query_params:
                            img_data["url_labels"] = query_params["label"]
                        if "percentage" in query_params:
                            img_data["url_percentages"] = query_params["percentage"]

                            # Try to extract percentages from URL parameters
                            if "extracted_data" not in img_data:
                                img_data["extracted_data"] = {}

                            percentages = query_params["percentage"].split(',')
                            labels = query_params.get("label", "").split(',')

                            if len(percentages) > 0:
                                # If we have labels, use them; otherwise use generic labels
                                if len(labels) == len(percentages):
                                    for i, (label, pct) in enumerate(zip(labels, percentages)):
                                        img_data["extracted_data"][label.strip()] = pct.strip()
                                else:
                                    for i, pct in enumerate(percentages):
                                        img_data["extracted_data"][f"Value {i + 1}"] = pct.strip()
                except Exception as e:
                    print(f"Error parsing URL parameters: {e}")

                image_data.append(img_data)

            if image_data:
                graph_data["images"] = image_data

            # ---- TEXT DATA EXTRACTION ----
            # Capture text data from the table or emphasized elements
            table_text = {}
            try:
                # Method 1: Check table cells for percentage data
                rows = table.find_elements(By.TAG_NAME, "tr")
                for row in rows:
                    cells = row.find_elements(By.TAG_NAME, "td")
                    for cell in cells:
                        cell_text = cell.text.strip()
                        # Look for retention-related content
                        if "retention" in cell_text.lower() and "%" in cell_text:
                            # Try to extract percentages
                            for pattern in data_patterns:
                                matches = re.findall(pattern, cell_text)
                                for match in matches:
                                    # Handle different pattern formats
                                    if match[0].isdigit() or match[0].replace('.', '', 1).isdigit():
                                        value, label = match
                                    else:
                                        label, value = match
                                    table_text[label.strip()] = f"{value}%"

                # Method 2: Look for emphasized text that might contain percentages
                for elem_type in ["strong", "b", "em", "i", "span"]:
                    for elem in table.find_elements(By.TAG_NAME, elem_type):
                        elem_text = elem.text.strip()
                        if "%" in elem_text:
                            # Try to extract percentages
                            for pattern in data_patterns:
                                matches = re.findall(pattern, elem_text)
                                for match in matches:
                                    # Handle different pattern formats
                                    if match[0].isdigit() or match[0].replace('.', '', 1).isdigit():
                                        value, label = match
                                    else:
                                        label, value = match
                                    table_text[label.strip()] = f"{value}%"
            except Exception as e:
                print(f"Error extracting text data: {e}")

            if table_text:
                graph_data["text_data"] = table_text

            return graph_data
        except Exception as e:
            print(f"Error extracting graph data: {e}")
            return {"error": str(e)}

    def _save_excel_data(self, institution_input):
        """Save all collected data to a single Excel file"""
        if not self.all_data:
            print("No data was collected.")
            return False

        try:
            # Create filename from institution name and search term
            filename = f"{institution_input['clean_name']}_{institution_input['clean_search']}_data.xlsx"

            # Convert all_data to DataFrame
            df = pd.DataFrame(self.all_data)

            # Create a Pandas Excel writer with XlsxWriter as the engine
            with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:
                # Write dataframe to sheet
                df.to_excel(writer, sheet_name="College Data", index=False)

                # Get workbook and worksheet objects
                workbook = writer.book
                worksheet = writer.sheets["College Data"]

                # Add autofilter
                worksheet.autofilter(0, 0, df.shape[0], df.shape[1] - 1)

                # Set column widths
                for i, column in enumerate(df.columns):
                    # Get the maximum length of the column name and values
                    max_len = max(
                        len(str(column)),
                        df[column].astype(str).map(len).max()
                    )
                    # Set the column width
                    worksheet.set_column(i, i, min(max_len + 2, 50))  # Max width of 50

                # Create a format for section headers
                header_format = workbook.add_format({
                    'bold': True,
                    'bg_color': '#D3D3D3',  # Light gray background
                    'font_size': 12
                })

                # Apply formatting to section header rows
                for row_idx, row in df.iterrows():
                    category = row['Category']
                    if category.startswith('---') or category == 'GENERAL INFORMATION':
                        # Apply formatting to this row
                        worksheet.set_row(row_idx + 1, None, header_format)  # +1 for header row

            print(f"\nAll data saved to {filename}")
            return True
        except Exception as e:
            print(f"Error saving Excel file: {e}")
            return False

    def _normalize_search_term(self, term):
        """Normalize a search term for better matching"""
        # Convert to lowercase
        term = term.lower()

        # Handle common variations
        replacements = {
            "6-year": "6 year",
            "6 year": "6-year",
            "six-year": "6-year",
            "six year": "6-year",
            "graduation": "grad",
            "graduation rate": "grad rate"
        }

        for old, new in replacements.items():
            if old in term:
                # Create alternative versions
                alt_terms = [term]
                alt_terms.append(term.replace(old, new))
                return alt_terms

        return [term]

    def _check_general_info_for_search(self, search_terms, keywords):
        """Check if the search terms or keywords are in general information"""
        found_matches = []

        # Loop through all general information items
        for key, value in self.institution_info.items():
            key_lower = key.lower()
            value_lower = str(value).lower()
            combined = f"{key_lower} {value_lower}"

            # Check for search terms
            for term in search_terms:
                if term.lower() in combined:
                    found_matches.append(f"{key}: {value}")
                    break

            # If not found by terms, check keywords
            if not any(term.lower() in combined for term in search_terms):
                if self._keyword_match(combined, keywords):
                    found_matches.append(f"{key}: {value}")

        return found_matches

    def _extract_keywords(self, search_terms):
        """Extract keywords from search terms for flexible matching"""
        keywords = []
        for term in search_terms:
            # Split into words and filter out very short words
            words = [word for word in term.split() if len(word) > 2]
            keywords.extend(words)

        # Remove duplicates
        return list(set(keywords))

    def _keyword_match(self, text, keywords):
        """Match text against a list of keywords, return True if all keywords are found"""
        text = text.lower()
        for keyword in keywords:
            if keyword.lower() not in text:
                return False
        return True

    def _clean_filename(self, text):
        """Clean text for use in filenames"""
        return re.sub(r'[^\w\s]', '', text).replace(' ', '_').lower()

    def _cleanup(self):
        """Close the browser and clean up resources"""
        try:
            # Ask user to press Enter before closing
            input("\nPress Enter to close the browser...")
        finally:
            if self.driver:
                self.driver.quit()
                print("Browser closed")


# Main execution
if __name__ == "__main__":
    scraper = CollegeNavigatorScraper()
    scraper.run()